# Model arguments
model_name_or_path: meta-llama/Llama-3.1-8B
torch_dtype: bfloat16

# Data arguments
dataset_mixer:
  my_mcq_dataset: 1.0
dataset_splits:
  - train
  # Optionally skip test
text_column: text

# SFT trainer config
num_train_epochs: 50
per_device_train_batch_size: 1
learning_rate: 1e-5
dropout: 0.0
weight_decay: 0.0
logging_steps: 1
max_seq_length: 512
output_dir: /tmp/overfit-mcq
