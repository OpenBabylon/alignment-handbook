# ---------------------------------------------------
# Model arguments
# ---------------------------------------------------
model_name_or_path: meta-llama/Llama-3.1-8B
torch_dtype: bfloat16
attn_implementation: flash_attention_2

# If you had quantization in Axolotl:
#   base_model.quantization_config.load_in_8bit: true
# Then in HF you'd do:
#quantization_config:
#  load_in_8bit: true

# ---------------------------------------------------
# Data training arguments
# ---------------------------------------------------
dataset_mixer:
  my_dataset_name: 1.0   # e.g. from your Axolotl "datasets" section
dataset_splits:
  - train
  - test
text_column: text
preprocessing_num_workers: 8

# ---------------------------------------------------
# SFT trainer config
# ---------------------------------------------------
bf16: true
do_eval: true
eval_strategy: steps
eval_steps: 500
gradient_accumulation_steps: 4
gradient_checkpointing: true
learning_rate: 1.0e-06
logging_steps: 5
lr_scheduler_type: cosine
max_seq_length: 2048
num_train_epochs: 3
output_dir: /my/hf/sft-output
overwrite_output_dir: true
per_device_eval_batch_size: 4
per_device_train_batch_size: 4
push_to_hub: false
save_steps: 500
seed: 42
warmup_ratio: 0.1
