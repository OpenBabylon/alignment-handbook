# Model arguments
# model_name_or_path: meta-llama/Llama-3.2-1B-Instruct
model_name_or_path: meta-llama/Meta-Llama-3-8B-Instruct
torch_dtype: bfloat16
attn_implementation: flash_attention_2

# Data training arguments
dataset_mixer:
  # ingvar/zno_instruction_mcq_dataset: 1.0
  ingvar/zno_syvokon: 1.0
dataset_splits:
  - train
  - test
preprocessing_num_workers: 4

# Training setup
bf16: true
do_eval: true
gradient_accumulation_steps: 2
learning_rate: 5.0e-06
max_seq_length: 1024
num_train_epochs: 10
per_device_train_batch_size: 5
logging_steps: 5
output_dir: /data/syvokon-8b_slow
push_to_hub: false
save_strategy: epoch
seed: 42

eval_save_dir: "eval_results"
lm_eval_tasks:
  - "zno-history"
  - "zno-literature"
limit: -1 

