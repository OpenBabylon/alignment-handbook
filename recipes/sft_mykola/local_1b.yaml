# Model arguments
model_name_or_path: meta-llama/Llama-3.2-1B-Instruct
torch_dtype: bfloat16
attn_implementation: flash_attention_2

# Data training arguments
dataset_mixer:
  ingvar/zno_syvokon: 1.0
  # PolyAgent/sft-qa-ukid-zno-historybook-chat: 1.0
dataset_splits:
  - train
  - test
preprocessing_num_workers: 4

# Training setup- 
bf16: true
# load_in_8bit: true
# do_eval: false
evaluation_strategy: steps
eval_steps: 40
gradient_accumulation_steps: 8
learning_rate: 5.0e-05
max_seq_length: 512
num_train_epochs: 7
per_device_train_batch_size: 2
logging_steps: 5
output_dir: /home/mykola/sft/output_models/test_repo
push_to_hub: false
save_strategy: steps
save_steps: 40
seed: 42


eval_save_dir: "eval_results"
lm_eval_tasks:
  - "zno-language"
limit: -1